{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PERFORM DATA INGESTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "unzip the  dataset : !unzip folderpath\n",
    "\n",
    "\n",
    "\n",
    "load the impbalanced dataset: pd.read_csv(imbalanced data path)\n",
    "\n",
    "\n",
    "\n",
    "        index     id       label        tweet\n",
    "          0       1           0         @user when a fther is dysfunctionala nd is a .....\n",
    "          1       2           0         @user@user  thanks foer the credit I cant us.....\n",
    "          2       3           0\n",
    "          3       4           0\n",
    "          4       5           0\n",
    "          5       6           1\n",
    "          6       7\n",
    "        \n",
    "        \n",
    "        \n",
    "        we dont need the id column "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  EDA\n",
    "\n",
    "    1.COUNT PLOT of the labels to know the number of each 0 and 1 labels: i is lesser than the number of 0s\n",
    "\n",
    "    2.shape of the dataset to know the number of rows and then columns: \n",
    "\n",
    "    3.check for the missing values in the dataset \n",
    "\n",
    "    4.drop the id column: df.drop(column,axix=1,inplace=True)\n",
    "\n",
    "    5. HAndle the imbalance datset by finding another source  of the data  maybe in the internet etc such as the row data which we are luck is in the same format as the imbalanced dataset\n",
    "    csv1(imbalanced data)                    csv 2(raw data)\n",
    "      o label 45                              o label 56\n",
    "      1 label 34                              1 lable 100\n",
    "    so perform merging of the label 1 in csv 2 into the lable 1 in csv 1, we cant do the missing values techniques we usually do when we have numerical data\n",
    "    and if we have less data we can perform augmentation but here we have imbalanced data so we cant perform augmentation  directly or we can first handle the the imbalanced problem \n",
    "    then perform augmentation\n",
    "\n",
    "\n",
    "    Our raw data looks like this\n",
    "                unnamed :0     count   hatespeech       effective_language  neither   class      tweet\n",
    "\n",
    "              0      0            3           0                   0              3          2        !!!RT@finaatieono as a woman you shouldnt\n",
    "              1                   3            0                  3                0          1\n",
    "              2                   0            1                  2                0          1\n",
    "              3\n",
    "              4\n",
    "              5\n",
    "  \n",
    "so we can choose the columns that will be relevant  such as the tweet  and the class and so drop the remaining columns\n",
    "Copy the  labeles of class 1 into class 0  in the raq data coz they are the same\n",
    "\n",
    "\n",
    "merge the two datdframes together rowise: frame[imbalanced, raw data] ; df= pd.concat(frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PREPROCESSING STEP\n",
    "\n",
    "use the nltk library to give you the functionality to preproccess your data\n",
    "imort nltk snowballStemmer and the stopwords\n",
    "import the regex\n",
    "1. create a function that will perfomr the following\n",
    "            convert to lowercase\n",
    "            usimg the \n",
    "            remove the punctuation marks: regex.sub('the puntuations you want to remove,what to replace the space formed when we removed the punctuation,the column to appy)\n",
    "            remove theurls tags: regex.sub(')\n",
    "            remove the tags\n",
    "            remove the stopwords in the data\n",
    "            join the words now using an empty space\n",
    "            then perform stemming \n",
    "            then join al the words now to give you the clean text\n",
    "\n",
    "2. split the dataset into x variables and the y variables\n",
    "\n",
    "3. perform the train  test split using the sklearn train test split function\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## perform the feature engineering\n",
    "\n",
    "## nb ##\n",
    "perform text vectorization  using either the tf_idf,bg of words, word to vec, embedding, but here we will use th e embedding LAYER from keras\n",
    "\n",
    ".Before passing the data to the embedding layer we need to perform  interior encoding of the data which includes \n",
    "     -Tokenization on the xtrain dataset , tokenizer from keras\n",
    "     -text to sequence on the xtrain dataset ie sequences=tokenizer.text_sequence(xtrain)\n",
    "     -pad the sequences using pad_sequence(sequence,....) so that you may fill the 'sentence' with short lengths to match the length of the of the longest sentence, so you can add zero values before or after\n",
    "     \n",
    "perform the embedding on the sequence matrix above  then just do the model trainig cusing the code :\n",
    "\n",
    "import keras.models import sequential,\n",
    "\n",
    "from keras.layers import LSTM,activation,dense,droput,input,embedding,spartialDropout\n",
    "\n",
    "from from kers,optimixer import RMSprop\n",
    "          model=sequential()\n",
    "\n",
    "          modlel.add(embeddin(max_words,100,inout_llength0-max_len))\n",
    "\n",
    "          model.add(spatialDropout1D(0.2))\n",
    "\n",
    "          model.add(LSTM(0100,droput=0/2.recurrent_dropout=0))\n",
    "\n",
    "          model.add(demse(1,activtion='signoid))\n",
    "\n",
    "          model.summary()\n",
    "\n",
    "          model.compile(loss='binary_crossentropy,optimser=RMSprop(),metrics=['accuracy'])\n",
    "\n",
    "          model.fit()\n",
    "\n",
    "          model.evaluate()\n",
    "          \n",
    "          save the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test the model on a new data\n",
    "\n",
    "data='I love the  @@ parentinf she has !! on her kids'\n",
    "\n",
    "        1. perform text cleaning on this testing data, same function we used in the cleaning our training data\n",
    "\n",
    "        2. peform the tokenization\n",
    "\n",
    "        3. perform the text to sequence on the tokenized sentence to form a ssequence matrix\n",
    "\n",
    "        4. load the model and predict ie load_model.predict(the sequence matrix) and save in  a variable\n",
    "\n",
    "        5. print the pred\n",
    "\n",
    "        6. if pred< 0.5 then print no hate  if pred > 0.5 then print hate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now add this colab notebook to your project folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#THE CODE BELOW is copy pasted from the egoogle colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_txt_clssfc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
